{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from helpers import *\n",
    "\n",
    "from crossvalidation import cross_validate\n",
    "from crossvalidation import tune_hyperparameters\n",
    "#from crossvalidation import tune_hyperparameters1\n",
    "#from crossvalidation import tune_hyperparameters2\n",
    "\n",
    "from helpers import *\n",
    "from helpers_perso import *\n",
    "from preprocessing.nan_imputation import *\n",
    "from preprocessing.binary_encoding import *\n",
    "from implementations import *\n",
    "from preprocessing.standardization import *\n",
    "from preprocessing.class_balancing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\", \"dataset\")\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "print(\"Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CODE\n",
    "# Create test arrays\n",
    "x_train = np.random.rand(150, 200)  # 25 x 25 array for features\n",
    "y_train = np.random.rand(150)   # 25 x 1 array for target values\n",
    "initial_weights = np.zeros(x_train.shape[1])  # Define initial weights based on feature count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_balanced, y_balanced, deleted_ids = balance_classes(x_train, y_train, 1)\n",
    "\n",
    "x_train_cleaned, deleted_indices = remove_nan_features(x_train, 0.8)\n",
    "adapted_x_test = np.delete(x_test, deleted_indices, axis=1)\n",
    "\n",
    "\n",
    "integer_columns, non_integer_columns = identify_integer_columns(x_train_cleaned)\n",
    "assert len(integer_columns) + len(non_integer_columns) == x_train_cleaned.shape[1]\n",
    "\n",
    "x_train_cleaned_without_nans = encode_nan_integer_columns(x_train_cleaned, replacement_value='mode')\n",
    "x_train_cleaned_without_nans = encode_nan_continuous_columns(x_train_cleaned_without_nans, replacement_value='mode')\n",
    "assert np.isnan(x_train_cleaned_without_nans).sum() == 0\n",
    "assert x_train_cleaned.shape == x_train_cleaned_without_nans.shape\n",
    "adapted_x_test_without_nans = encode_nan_integer_columns(adapted_x_test, replacement_value='mode')\n",
    "adapted_x_test_without_nans = encode_nan_continuous_columns(adapted_x_test_without_nans, replacement_value='mode')\n",
    "assert np.isnan(adapted_x_test_without_nans).sum() == 0\n",
    "assert adapted_x_test.shape == adapted_x_test_without_nans.shape\n",
    "\n",
    "categorical_threshold = 5\n",
    "unique_value_counts = np.array([len(np.unique(x_train_cleaned[:, col])) for col in integer_columns])\n",
    "indexes_categorical_features = [integer_columns[i] for i, count in enumerate(unique_value_counts) if count <= categorical_threshold]\n",
    "indexes_non_categorical_features = [integer_columns[i] for i in range(len(unique_value_counts)) if integer_columns[i] not in indexes_categorical_features]\n",
    "assert len(indexes_categorical_features) + len(indexes_non_categorical_features) == len(unique_value_counts)\n",
    "assert unique_value_counts.size == len(integer_columns)\n",
    "indexes_non_categorical_features.extend(non_integer_columns)\n",
    "\n",
    "x_standardized = standardize_columns(x_train_cleaned_without_nans, indexes_non_categorical_features)\n",
    "x_test_standardized = standardize_columns(adapted_x_test_without_nans, indexes_non_categorical_features)\n",
    "\n",
    "encoded_x_train, encoded_x_test = consistent_binary_encode(x_standardized, x_test_standardized, indexes_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs in x_train:\", np.isnan(x_train).any())\n",
    "print(\"Infs in x_train:\", np.isinf(x_train).any())\n",
    "print(\"NaNs in y_train:\", np.isnan(y_train).any())\n",
    "print(\"Infs in y_train:\", np.isinf(y_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models and Hyperparameter Grids\n",
    "# Dictionary of model functions\n",
    "models = {\n",
    "    # \"mean_squared_error_gd\": mean_squared_error_gd,\n",
    "    # \"mean_squared_error_sgd\": mean_squared_error_sgd,\n",
    "    # \"least_squares\": least_squares,\n",
    "    # \"ridge_regression\": ridge_regression,\n",
    "    \"logistic_regression\": logistic_regression,\n",
    "    # \"reg_logistic_regression\": reg_logistic_regression\n",
    "}\n",
    "\n",
    "# # Define hyperparameter grids for each model\n",
    "param_grid = {\n",
    "    \"mean_squared_error_gd\": {\"max_iters\": [15], \"gamma\": np.linspace(0.0001, 0.00001, 5).tolist()},\n",
    "    \"mean_squared_error_sgd\": {\"max_iters\": [15], \"gamma\": np.linspace(0.0001, 0.00001, 5).tolist()},\n",
    "    \"least_squares\": {},  # No hyperparameters for least squares\n",
    "    \"ridge_regression\": {\"lambda_\": [0.2, 0.1, 0.01, 0.001, 0.0001]},\n",
    "    \"logistic_regression\": {\"max_iters\": [15], \"gamma\": np.linspace(0.1, 0.00001, 100).tolist()},\n",
    "    \"reg_logistic_regression\": {\"max_iters\": [15], \"gamma\": np.linspace(0.0001, 0.00001, 5).tolist(), \"lambda_\": [0.2, 0.1, 0.01, 0.001, 0.0001]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning\n",
    "# Define models and parameter grid (as done before)\n",
    "# Example usage of the tune_hyperparameters function\n",
    "x_train = encoded_x_train\n",
    "initial_weights = np.zeros(x_train.shape[1])  # Initial weights\n",
    "\n",
    "# TODO create loss for regression (-log)\n",
    "\n",
    "tuning_results = tune_hyperparameters(models, param_grid, x_train, y_train, initial_weights, k=5)\n",
    "#tuning_results = tune_hyperparameters1(models, param_grid, x_train, y_train, initial_weights, k=5)\n",
    "#tuning_results = tune_hyperparameters2(models, param_grid, x_train, y_train, k=5)\n",
    "\n",
    "# Print best results for each model\n",
    "for model_name, result in tuning_results.items():\n",
    "    print(f\"{model_name}: Best params: {result['best_params']}, Best score: {result['best_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) define the parameters\n",
    "initial_w = np.zeros(x_train.shape[1])  # Initial weights\n",
    "max_iters = 100                       # Number of iterations\n",
    "gamma = 0.1                             # Learning rate\n",
    "lambda_ = 0.1                           # Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) mean_squared_error_gd\n",
    "cv_loss_gd = cross_validate(model_fn=mean_squared_error_gd, X=x_train, y=y_train, k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma)\n",
    "print(\"Cross-validated loss for mean_squared_error_gd:\", cv_loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code que j'avais fais avant qd c'était dans le run, \n",
    "# mtn méthode differente, car run de crossvalidation dans le notebook crossvalidation\n",
    "\n",
    "\n",
    "# ***************** Cross validation ***************** #\n",
    "\n",
    "# Flexible crossvalidation on which function from implementations you want to validate\n",
    "# there are dynamical arguments and you can change the function you want to validate by changing the function name\n",
    "\n",
    "# 0) define the parameters\n",
    "initial_w = np.zeros(x_train.shape[1])  # Initial weights\n",
    "max_iters = 100                       # Number of iterations\n",
    "gamma = 0.1                             # Learning rate\n",
    "lambda_ = 0.1                           # Regularization parameter\n",
    "\n",
    "# 1) mean_squared_error_gd\n",
    "cv_loss_gd = cross_validate(model_fn=mean_squared_error_gd, X=x_train, y=y_train, k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma)\n",
    "print(\"Cross-validated loss for mean_squared_error_gd:\", cv_loss_gd)\n",
    "\n",
    "# 2) mean_squared_error_sgd\n",
    "cv_loss_sgd = cross_validate(model_fn=mean_squared_error_sgd, X=x_train, y=y_train, k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma)\n",
    "print(\"Cross-validated loss for mean_squared_error_sgd:\", cv_loss_gd)\n",
    "\n",
    "# 3) least_squares\n",
    "cv_loss_ls = cross_validate(model_fn=least_squares, X=x_train, Y=y_train, k=5)\n",
    "print(\"Cross-validated loss for least_squares:\", cv_loss_ls)\n",
    "\n",
    "# 4) ridge_regression\n",
    "#lambda_ = 0.1\n",
    "#cv_loss_rr = cross_validate(ridge_regression, X=x_train, Y=y_train, k=5, lambda_=lambda_)\n",
    "#print(\"Cross-validated loss for ridge_regression:\", cv_loss_rr)\n",
    "\n",
    "# 5) logistic_regression\n",
    "cv_loss_lr = cross_validate(\n",
    "    model_fn=logistic_regression,  # Pass the logistic regression function\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    initial_w=initial_w,\n",
    "    max_iters=max_iters,\n",
    "    gamma=gamma\n",
    "    )\n",
    "print(\"Cross-validated loss for logistic_regression:\", cv_loss_lr)\n",
    "\n",
    "# 6) reg_logistic_regression\n",
    "cv_loss_rlr = cross_validate(\n",
    "    model_fn=reg_logistic_regression,  # Pass the regularized logistic regression function\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma,lambda_=lambda_\n",
    ")\n",
    "print(\"Cross-validated loss for reg_logistic_regression:\", cv_loss_rlr)\n",
    "\n",
    "# 4) Ridge Regression : cross validate + fine tuning of parameters\n",
    "# Define ranges for hyperparameters\n",
    "gamma_values = [0.01, 0.1, 0.5]\n",
    "lambda_values = [0.01, 0.1, 1, 10]\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform grid search over gamma and lambda_\n",
    "for gamma in gamma_values:\n",
    "    for lambda_ in lambda_values:\n",
    "        cv_loss_rr = cross_validate(model_fn=ridge_regression,X=x_train,y=y_train,k=5,gamma=gamma,lambda_=lambda_)\n",
    "        print(f\"Cross-validated loss with gamma={gamma}, lambda_={lambda_}: {cv_loss_rr}\")\n",
    "        \n",
    "        # Update best score and parameters\n",
    "        if cv_loss_rr < best_score:\n",
    "            best_score = cv_loss_rr\n",
    "            best_params = {'gamma': gamma, 'lambda_': lambda_}\n",
    "\n",
    "print(f\"The best parameters: {best_params} yield a cross-validated loss of: {best_score}\")\n",
    "\n",
    "\n",
    "# LOOCV: To perform Leave-One-Out Cross-Validation --> just call cross_validate with k=len(y)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
