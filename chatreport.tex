\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}

\title{Machine Learning Project 1 Report}
\author{AI Huasca Team}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
In this project, we address a machine learning classification task with a large dataset, aiming to achieve accurate predictions on unseen data. Our approach focuses on systematic data preprocessing, model selection, and evaluation to maximize the predictive performance. We used a range of machine learning techniques, from linear regression to logistic regression, and optimized our models to handle real-world imperfections like missing values and high-dimensional feature space.

\section{Data Preprocessing}
The raw dataset contained missing values and categorical as well as continuous features. Our preprocessing steps included:
\begin{itemize}
    \item \textbf{Handling Missing Values:} We removed columns containing more than 80\% NaN values, as these columns offered limited information. For the remaining missing values, we applied mode imputation for integer columns and continuous columns to fill gaps reliably without introducing bias.
    \item \textbf{One-Hot Encoding:} To handle categorical variables, we used one-hot encoding on columns with a limited number of unique values, effectively transforming them into binary features.
    \item \textbf{Standardization:} Continuous features were standardized to ensure that all features had a mean of 0 and a standard deviation of 1, which is essential for the convergence of gradient-based optimization techniques.
\end{itemize}

\section{Model Selection and Implementation}
We explored several models suitable for both regression and classification tasks:
\begin{itemize}
    \item \textbf{Linear Regression (Gradient Descent):} Used for initial explorations, focusing on mean squared error as the loss function. Gradient descent helped in optimizing weights efficiently with a controlled learning rate.
    \item \textbf{Stochastic Gradient Descent (SGD):} This model was particularly useful for large datasets, as it updates weights with each data point, offering computational efficiency and faster convergence.
    \item \textbf{Ridge Regression:} Implemented to handle multicollinearity, introducing a regularization term to penalize large coefficients, thereby improving generalization.
    \item \textbf{Logistic Regression and Regularized Logistic Regression:} Given the binary nature of our target, logistic regression was an appropriate choice for classification. Regularization (L2) was added to prevent overfitting by penalizing overly complex models.
\end{itemize}

\section{Evaluation and Results}
We assessed our models using cross-validation, focusing on metrics like accuracy, precision, and mean squared error for regression-based models. 
\begin{itemize}
    \item \textbf{Cross-Validation:} 5-fold cross-validation helped in obtaining robust accuracy estimates while avoiding overfitting.
    \item \textbf{Final Model Selection:} Based on cross-validation results, regularized logistic regression provided the best balance of accuracy and generalization on the test set, outperforming other models in terms of predictive performance.
\end{itemize}

The final predictions were generated with the optimized logistic regression model, achieving satisfactory accuracy on the test set. The submission was prepared in the required format with predicted labels as -1 and 1.

\section{Conclusion}
Our methodology involved systematic data cleaning, feature engineering, and model selection. Regularized logistic regression proved to be the best model for this classification task. Future work could include exploring ensemble methods or deep learning approaches if computational resources permit.

\end{document}
