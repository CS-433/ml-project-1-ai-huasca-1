{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from helpers import *\n",
    "from helpers_perso import *\n",
    "from nan_imputation import *\n",
    "from one_hot_encoding import *\n",
    "from implementations import *\n",
    "from standardization import *\n",
    "from class_balancing import *\n",
    "from remove_highly_correlated_features import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\", \"dataset\")\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "print(\"Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "column_variances = np.nanvar(x_train, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# First subplot: Full range of variances with more bins\n",
    "axes[0].hist(column_variances, bins=100, edgecolor='black')\n",
    "axes[0].set_title(\"Distribution of Variance Across Columns (Full Range)\")\n",
    "axes[0].set_xlabel(\"Variance\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Second subplot: Focus on variance between 0 and 0.5\n",
    "axes[1].hist(column_variances, bins=20, range=(0, 1000), edgecolor='black')\n",
    "axes[1].set_title(\"Distribution of Variance Across Columns (0 to 1000)\")\n",
    "axes[1].set_xlabel(\"Variance\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Third subplot: Focus on variance between 1 and 200\n",
    "axes[2].hist(column_variances, bins=100, range=(1, 200), edgecolor='black')\n",
    "axes[2].set_title(\"Distribution of Variance Across Columns (1 to 200)\")\n",
    "axes[2].set_xlabel(\"Variance\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class1_ids = np.where(y_train == -1)[0]\n",
    "\n",
    "x_train_majority_class = x_train[class1_ids]\n",
    "\n",
    "# Calculate the proportion of NaN values in each column\n",
    "nan_proportions = np.isnan(x_train_majority_class).mean(axis=1)\n",
    "\n",
    "# Print the total number of columns plotted\n",
    "total_columns = nan_proportions.size\n",
    "print(f\"Total number of columns plotted: {total_columns}\")\n",
    "\n",
    "# Print the number of columns containing NaN values\n",
    "num_columns_with_nans = np.sum(nan_proportions > 0)\n",
    "print(f\"Number of columns containing NaN values: {num_columns_with_nans}\")\n",
    "\n",
    "# Define the bins for the histogram\n",
    "bins = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "\n",
    "# Calculate the histogram\n",
    "hist, bin_edges = np.histogram(nan_proportions, bins=bins)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(range(len(hist)), hist, tick_label=[f'{int(b*100)}-{int(bins[i+1]*100)}%' for i, b in enumerate(bins[:-1])])\n",
    "plt.xlabel('Proportion of NaN values')\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Number of columns containing a proportion of NaN values')\n",
    "\n",
    "# Rotate the x-axis tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321)\n",
      "(63745, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_balanced, y_balanced, deleted_ids = balance_classes(x_train, y_train, 1.2)\n",
    "print(x_balanced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling columns containing Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of Nan values in Nan-containing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the proportion of NaN values in each column\n",
    "nan_proportions = np.isnan(x_train).mean(axis=0)\n",
    "\n",
    "# Print the total number of columns plotted\n",
    "total_columns = nan_proportions.size\n",
    "print(f\"Total number of columns plotted: {total_columns}\")\n",
    "\n",
    "# Print the number of columns containing NaN values\n",
    "num_columns_with_nans = np.sum(nan_proportions > 0)\n",
    "print(f\"Number of columns containing NaN values: {num_columns_with_nans}\")\n",
    "\n",
    "# Define the bins for the histogram\n",
    "bins = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "\n",
    "# Calculate the histogram\n",
    "hist, bin_edges = np.histogram(nan_proportions, bins=bins)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(range(len(hist)), hist, tick_label=[f'{int(b*100)}-{int(bins[i+1]*100)}%' for i, b in enumerate(bins[:-1])])\n",
    "plt.xlabel('Proportion of NaN values')\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Number of columns containing a proportion of NaN values')\n",
    "\n",
    "# Rotate the x-axis tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus not reasonnable to exclude columns containing Nan values\n",
    "Choice : remove columns with Nan proportion superior to 80 % ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of columns to delete (NaN proportion superior to 80.0 %): 38.63%\n",
      "Data cleaned successfully!\n",
      "Original shape of x_train: (63745, 321)\n",
      "Cleaned shape of x_train: (63745, 197)\n"
     ]
    }
   ],
   "source": [
    "# Clean all arrays by removing columns containing NaN values\n",
    "x_train_cleaned, deleted_indices = remove_nan_features(x_balanced, 0.8)\n",
    "\n",
    "adapted_x_test = np.delete(x_train, deleted_indices, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns containing only integer values: 175\n",
      "Percentage of integer columns that contain at least one zero: 13.71%\n"
     ]
    }
   ],
   "source": [
    "integer_columns, non_integer_columns = identify_integer_columns(x_train_cleaned)\n",
    "\n",
    "assert len(integer_columns) + len(non_integer_columns) == x_train_cleaned.shape[1]\n",
    "\n",
    "\n",
    "# Print the integer columns\n",
    "print(f\"Number of columns containing only integer values: {len(integer_columns)}\")\n",
    "\n",
    "# Count the number of columns in integer_columns that contain at least one zero\n",
    "num_columns_with_zero = sum(np.any(x_train_cleaned[:, col] == 0) for col in integer_columns)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of integer columns that contain at least one zero: {num_columns_with_zero/len(integer_columns)*100:.2f}%\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If only contains integers and no zeroes (=encoded), encode Nan as 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 9 has been encoded with NaNs as the mode 1.0\n",
      "Column 10 has been encoded with NaNs as the mode 1.0\n",
      "Column 11 has been encoded with NaNs as the mode 1.0\n",
      "Column 12 has been encoded with NaNs as the mode 2.0\n",
      "Column 13 has been encoded with NaNs as the mode 2.0\n",
      "Column 14 has been encoded with NaNs as the mode 1.0\n",
      "Column 15 has been encoded with NaNs as the mode 1.0\n",
      "Column 16 has been encoded with NaNs as the mode 1.0\n",
      "Column 17 has been encoded with NaNs as the mode 1.0\n",
      "Column 18 has been encoded with NaNs as the mode 1.0\n",
      "Column 19 has been encoded with NaNs as the mode 1.0\n",
      "Column 20 has been encoded with NaNs as the mode 1.0\n",
      "Column 21 has been encoded with NaNs as the mode 2.0\n",
      "Column 22 has been encoded with NaNs as the mode 2.0\n",
      "Column 24 has been encoded with NaNs as the mode 88.0\n",
      "Column 26 has been encoded with NaNs as the mode 88.0\n",
      "Column 31 has been encoded with NaNs as the mode 1.0\n",
      "Column 32 has been encoded with NaNs as the mode 1.0\n",
      "Column 34 has been encoded with NaNs as the mode 1.0\n",
      "Column 35 has been encoded with NaNs as the mode 1.0\n",
      "Column 38 has been encoded with NaNs as the mode 2.0\n",
      "Column 44 has been encoded with NaNs as the mode 3.0\n",
      "Column 49 has been encoded with NaNs as the mode 2.0\n",
      "Column 50 has been encoded with NaNs as the mode 1.0\n",
      "Column 51 has been encoded with NaNs as the mode 2.0\n",
      "Column 53 has been encoded with NaNs as the mode 88.0\n",
      "Column 54 has been encoded with NaNs as the mode 8.0\n",
      "Column 55 has been encoded with NaNs as the mode 1.0\n",
      "Column 56 has been encoded with NaNs as the mode 9999.0\n",
      "Column 57 has been encoded with NaNs as the mode 506.0\n",
      "Column 58 has been encoded with NaNs as the mode 2.0\n",
      "Column 59 has been encoded with NaNs as the mode 2.0\n",
      "Column 60 has been encoded with NaNs as the mode 2.0\n",
      "Column 61 has been encoded with NaNs as the mode 2.0\n",
      "Column 62 has been encoded with NaNs as the mode 2.0\n",
      "Column 63 has been encoded with NaNs as the mode 2.0\n",
      "Column 64 has been encoded with NaNs as the mode 2.0\n",
      "Column 65 has been encoded with NaNs as the mode 1.0\n",
      "Column 66 has been encoded with NaNs as the mode 3.0\n",
      "Column 67 has been encoded with NaNs as the mode 7.0\n",
      "Column 68 has been encoded with NaNs as the mode 3.0\n",
      "Column 69 has been encoded with NaNs as the mode 888.0\n",
      "Column 70 has been encoded with NaNs as the mode 1.0\n",
      "Column 71 has been encoded with NaNs as the mode 88.0\n",
      "Column 72 has been encoded with NaNs as the mode 1.0\n",
      "Column 73 has been encoded with NaNs as the mode 555.0\n",
      "Column 74 has been encoded with NaNs as the mode 101.0\n",
      "Column 75 has been encoded with NaNs as the mode 555.0\n",
      "Column 76 has been encoded with NaNs as the mode 555.0\n",
      "Column 77 has been encoded with NaNs as the mode 555.0\n",
      "Column 78 has been encoded with NaNs as the mode 101.0\n",
      "Column 79 has been encoded with NaNs as the mode 2.0\n",
      "Column 80 has been encoded with NaNs as the mode 64.0\n",
      "Column 81 has been encoded with NaNs as the mode 107.0\n",
      "Column 82 has been encoded with NaNs as the mode 30.0\n",
      "Column 83 has been encoded with NaNs as the mode 88.0\n",
      "Column 84 has been encoded with NaNs as the mode 888.0\n",
      "Column 85 has been encoded with NaNs as the mode 1.0\n",
      "Column 86 has been encoded with NaNs as the mode 2.0\n",
      "Column 87 has been encoded with NaNs as the mode 3.0\n",
      "Column 88 has been encoded with NaNs as the mode 5.0\n",
      "Column 89 has been encoded with NaNs as the mode 1.0\n",
      "Column 90 has been encoded with NaNs as the mode 2.0\n",
      "Column 91 has been encoded with NaNs as the mode 102014.0\n",
      "Column 92 has been encoded with NaNs as the mode 1.0\n",
      "Column 93 has been encoded with NaNs as the mode 1.0\n",
      "Column 94 has been encoded with NaNs as the mode 2.0\n",
      "Column 95 has been encoded with NaNs as the mode 1.0\n",
      "Column 96 has been encoded with NaNs as the mode 4.0\n",
      "Column 98 has been encoded with NaNs as the mode 1.0\n",
      "Column 99 has been encoded with NaNs as the mode 1.0\n",
      "Column 104 has been encoded with NaNs as the mode 9.0\n",
      "Column 112 has been encoded with NaNs as the mode 2.0\n",
      "Column 116 has been encoded with NaNs as the mode 2.0\n",
      "Column 123 has been encoded with NaNs as the mode 1.0\n",
      "Column 128 has been encoded with NaNs as the mode 66.0\n",
      "Column 132 has been encoded with NaNs as the mode 3.0\n",
      "Column 167 has been encoded with NaNs as the mode 2.0\n",
      "Column 168 has been encoded with NaNs as the mode 0.0\n",
      "Column 169 has been encoded with NaNs as the mode 30.0\n",
      "Column 171 has been encoded with NaNs as the mode 210.0\n",
      "Column 172 has been encoded with NaNs as the mode 0.0\n",
      "Column 175 has been encoded with NaNs as the mode 420.0\n",
      "Column 176 has been encoded with NaNs as the mode 0.0\n",
      "Column 177 has been encoded with NaNs as the mode 0.0\n",
      "Column 178 has been encoded with NaNs as the mode 0.0\n",
      "Column 179 has been encoded with NaNs as the mode 0.0\n",
      "Column 180 has been encoded with NaNs as the mode 0.0\n",
      "Column 189 has been encoded with NaNs as the mode 3.0\n",
      "Column 190 has been encoded with NaNs as the mode 3.0\n",
      "Column 191 has been encoded with NaNs as the mode 4.0\n",
      "Column 194 has been encoded with NaNs as the mode 1.0\n",
      "Column 195 has been encoded with NaNs as the mode 1.0\n",
      "Column 196 has been encoded with NaNs as the mode 2.0\n",
      "Number of integer columns encoded: 94\n",
      "Column 101 has been encoded with NaNs as the binned mode 12.962805898798134\n",
      "Column 106 has been encoded with NaNs as the binned mode 0.754440299787147\n",
      "Column 129 has been encoded with NaNs as the binned mode 1.67875\n",
      "Column 130 has been encoded with NaNs as the binned mode 90.74804999999999\n",
      "Column 131 has been encoded with NaNs as the binned mode 27.32625\n",
      "Column 144 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 145 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 146 has been encoded with NaNs as the binned mode 0.125\n",
      "Column 147 has been encoded with NaNs as the binned mode 0.075\n",
      "Column 148 has been encoded with NaNs as the binned mode 0.0375\n",
      "Column 149 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 154 has been encoded with NaNs as the binned mode 0.2525\n",
      "Column 155 has been encoded with NaNs as the binned mode 1.3800000000000001\n",
      "Column 163 has been encoded with NaNs as the binned mode 3.488\n",
      "Column 164 has been encoded with NaNs as the binned mode 0.032\n",
      "Column 170 has been encoded with NaNs as the binned mode 6.899772499999999\n",
      "Column 173 has been encoded with NaNs as the binned mode 0.2475\n",
      "Number of non integer columns encoded: 17\n",
      "Column 9 has been encoded with NaNs as the mode 1.0\n",
      "Column 10 has been encoded with NaNs as the mode 1.0\n",
      "Column 11 has been encoded with NaNs as the mode 1.0\n",
      "Column 12 has been encoded with NaNs as the mode 2.0\n",
      "Column 13 has been encoded with NaNs as the mode 2.0\n",
      "Column 14 has been encoded with NaNs as the mode 1.0\n",
      "Column 15 has been encoded with NaNs as the mode 1.0\n",
      "Column 16 has been encoded with NaNs as the mode 1.0\n",
      "Column 17 has been encoded with NaNs as the mode 1.0\n",
      "Column 18 has been encoded with NaNs as the mode 2.0\n",
      "Column 19 has been encoded with NaNs as the mode 1.0\n",
      "Column 20 has been encoded with NaNs as the mode 1.0\n",
      "Column 21 has been encoded with NaNs as the mode 2.0\n",
      "Column 22 has been encoded with NaNs as the mode 2.0\n",
      "Column 23 has been encoded with NaNs as the mode 2.0\n",
      "Column 24 has been encoded with NaNs as the mode 88.0\n",
      "Column 26 has been encoded with NaNs as the mode 88.0\n",
      "Column 29 has been encoded with NaNs as the mode 2.0\n",
      "Column 30 has been encoded with NaNs as the mode 1.0\n",
      "Column 31 has been encoded with NaNs as the mode 3.0\n",
      "Column 32 has been encoded with NaNs as the mode 1.0\n",
      "Column 34 has been encoded with NaNs as the mode 1.0\n",
      "Column 35 has been encoded with NaNs as the mode 2.0\n",
      "Column 38 has been encoded with NaNs as the mode 2.0\n",
      "Column 41 has been encoded with NaNs as the mode 2.0\n",
      "Column 44 has been encoded with NaNs as the mode 3.0\n",
      "Column 49 has been encoded with NaNs as the mode 2.0\n",
      "Column 50 has been encoded with NaNs as the mode 1.0\n",
      "Column 51 has been encoded with NaNs as the mode 2.0\n",
      "Column 53 has been encoded with NaNs as the mode 88.0\n",
      "Column 54 has been encoded with NaNs as the mode 8.0\n",
      "Column 55 has been encoded with NaNs as the mode 1.0\n",
      "Column 56 has been encoded with NaNs as the mode 200.0\n",
      "Column 57 has been encoded with NaNs as the mode 504.0\n",
      "Column 58 has been encoded with NaNs as the mode 2.0\n",
      "Column 59 has been encoded with NaNs as the mode 2.0\n",
      "Column 60 has been encoded with NaNs as the mode 2.0\n",
      "Column 61 has been encoded with NaNs as the mode 2.0\n",
      "Column 62 has been encoded with NaNs as the mode 2.0\n",
      "Column 63 has been encoded with NaNs as the mode 2.0\n",
      "Column 64 has been encoded with NaNs as the mode 2.0\n",
      "Column 65 has been encoded with NaNs as the mode 2.0\n",
      "Column 66 has been encoded with NaNs as the mode 3.0\n",
      "Column 67 has been encoded with NaNs as the mode 7.0\n",
      "Column 68 has been encoded with NaNs as the mode 3.0\n",
      "Column 69 has been encoded with NaNs as the mode 888.0\n",
      "Column 70 has been encoded with NaNs as the mode 1.0\n",
      "Column 71 has been encoded with NaNs as the mode 88.0\n",
      "Column 72 has been encoded with NaNs as the mode 2.0\n",
      "Column 73 has been encoded with NaNs as the mode 555.0\n",
      "Column 74 has been encoded with NaNs as the mode 101.0\n",
      "Column 75 has been encoded with NaNs as the mode 555.0\n",
      "Column 76 has been encoded with NaNs as the mode 101.0\n",
      "Column 77 has been encoded with NaNs as the mode 555.0\n",
      "Column 78 has been encoded with NaNs as the mode 101.0\n",
      "Column 79 has been encoded with NaNs as the mode 1.0\n",
      "Column 80 has been encoded with NaNs as the mode 64.0\n",
      "Column 81 has been encoded with NaNs as the mode 103.0\n",
      "Column 82 has been encoded with NaNs as the mode 30.0\n",
      "Column 83 has been encoded with NaNs as the mode 88.0\n",
      "Column 84 has been encoded with NaNs as the mode 888.0\n",
      "Column 85 has been encoded with NaNs as the mode 2.0\n",
      "Column 86 has been encoded with NaNs as the mode 2.0\n",
      "Column 87 has been encoded with NaNs as the mode 3.0\n",
      "Column 88 has been encoded with NaNs as the mode 5.0\n",
      "Column 89 has been encoded with NaNs as the mode 1.0\n",
      "Column 90 has been encoded with NaNs as the mode 2.0\n",
      "Column 91 has been encoded with NaNs as the mode 102014.0\n",
      "Column 92 has been encoded with NaNs as the mode 1.0\n",
      "Column 93 has been encoded with NaNs as the mode 2.0\n",
      "Column 94 has been encoded with NaNs as the mode 2.0\n",
      "Column 95 has been encoded with NaNs as the mode 1.0\n",
      "Column 96 has been encoded with NaNs as the mode 4.0\n",
      "Column 98 has been encoded with NaNs as the mode 1.0\n",
      "Column 99 has been encoded with NaNs as the mode 1.0\n",
      "Column 104 has been encoded with NaNs as the mode 9.0\n",
      "Column 112 has been encoded with NaNs as the mode 1.0\n",
      "Column 116 has been encoded with NaNs as the mode 2.0\n",
      "Column 123 has been encoded with NaNs as the mode 1.0\n",
      "Column 128 has been encoded with NaNs as the mode 64.0\n",
      "Column 132 has been encoded with NaNs as the mode 3.0\n",
      "Column 167 has been encoded with NaNs as the mode 1.0\n",
      "Column 168 has been encoded with NaNs as the mode 0.0\n",
      "Column 169 has been encoded with NaNs as the mode 30.0\n",
      "Column 171 has been encoded with NaNs as the mode 180.0\n",
      "Column 172 has been encoded with NaNs as the mode 0.0\n",
      "Column 175 has been encoded with NaNs as the mode 0.0\n",
      "Column 176 has been encoded with NaNs as the mode 0.0\n",
      "Column 177 has been encoded with NaNs as the mode 180.0\n",
      "Column 178 has been encoded with NaNs as the mode 0.0\n",
      "Column 179 has been encoded with NaNs as the mode 0.0\n",
      "Column 180 has been encoded with NaNs as the mode 0.0\n",
      "Column 189 has been encoded with NaNs as the mode 3.0\n",
      "Column 190 has been encoded with NaNs as the mode 3.0\n",
      "Column 191 has been encoded with NaNs as the mode 4.0\n",
      "Column 194 has been encoded with NaNs as the mode 1.0\n",
      "Column 195 has been encoded with NaNs as the mode 1.0\n",
      "Column 196 has been encoded with NaNs as the mode 2.0\n",
      "Number of integer columns encoded: 98\n",
      "Column 101 has been encoded with NaNs as the binned mode 12.962805898798134\n",
      "Column 106 has been encoded with NaNs as the binned mode 0.754440299787147\n",
      "Column 129 has been encoded with NaNs as the binned mode 1.63375\n",
      "Column 130 has been encoded with NaNs as the binned mode 90.140425\n",
      "Column 131 has been encoded with NaNs as the binned mode 24.222275000000003\n",
      "Column 144 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 145 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 146 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 147 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 148 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 149 has been encoded with NaNs as the binned mode 0.2475\n",
      "Column 154 has been encoded with NaNs as the binned mode 0.375\n",
      "Column 155 has been encoded with NaNs as the binned mode 1.494675\n",
      "Column 163 has been encoded with NaNs as the binned mode 3.488\n",
      "Column 164 has been encoded with NaNs as the binned mode 0.032\n",
      "Column 170 has been encoded with NaNs as the binned mode 2.9490925\n",
      "Column 173 has been encoded with NaNs as the binned mode 0.2475\n",
      "Number of non integer columns encoded: 17\n"
     ]
    }
   ],
   "source": [
    "x_train_cleaned_without_nans = encode_nan_integer_columns(x_train_cleaned, replacement_value='mode')\n",
    "x_train_cleaned_without_nans = encode_nan_continuous_columns(x_train_cleaned_without_nans, replacement_value='mode')\n",
    "\n",
    "assert np.isnan(x_train_cleaned_without_nans).sum() == 0\n",
    "assert x_train_cleaned.shape == x_train_cleaned_without_nans.shape\n",
    "\n",
    "adapted_x_test_without_nans = encode_nan_integer_columns(adapted_x_test, replacement_value='mode')\n",
    "adapted_x_test_without_nans = encode_nan_continuous_columns(adapted_x_test_without_nans, replacement_value='mode')\n",
    "\n",
    "assert np.isnan(adapted_x_test_without_nans).sum() == 0\n",
    "assert adapted_x_test.shape == adapted_x_test_without_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of columns that do not contain only integer values\n",
    "num_non_integer_columns = len(non_integer_columns)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of columns that do not contain only integer values: {num_non_integer_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In columns containing only integer values, number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_value_counts = np.array([len(np.unique(x_train_cleaned[:, col])) for col in integer_columns])\n",
    "\n",
    "# Create 20 bins based on the range of unique value counts\n",
    "max_unique = unique_value_counts.max() if unique_value_counts.size > 0 else 0\n",
    "bins = np.linspace(0, max_unique, 21)  # 21 edges for 20 bins\n",
    "bin_labels = [f'{int(b)}-{int(bins[i+1])}' for i, b in enumerate(bins[:-1])]\n",
    "\n",
    "# Count how many columns fall into each bin\n",
    "binned_counts = np.histogram(unique_value_counts, bins=bins)[0]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_labels, binned_counts, width=0.6, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Unique Values in Columns')\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.title('Columns Grouped by Number of Unique Values (20 Bins)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "# Create the second set of bins for values between 2000 and 0\n",
    "bins_2000 = np.linspace(0, 2000, 21)  # 21 edges for 20 bins\n",
    "bin_labels_2000 = [f'{int(b)}-{int(bins_2000[i+1])}' for i, b in enumerate(bins_2000[:-1])]\n",
    "\n",
    "# Count how many columns fall into each bin for the second set of bins\n",
    "binned_counts_2000 = np.histogram(unique_value_counts, bins=bins_2000)[0]\n",
    "\n",
    "# Create the second bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_labels_2000, binned_counts_2000, width=0.6, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Unique Values in Columns (0-2000)')\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.title('Columns Grouped by Number of Unique Values (20 Bins, 0-2000)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create the third set of bins for values between 0 and 100\n",
    "bins_100 = np.linspace(0, 100, 21)  # 21 edges for 20 bins\n",
    "bin_labels_100 = [f'{int(b)}-{int(bins_100[i+1])}' for i, b in enumerate(bins_100[:-1])]\n",
    "\n",
    "# Count how many columns fall into each bin for the third set of bins\n",
    "binned_counts_100 = np.histogram(unique_value_counts, bins=bins_100)[0]\n",
    "\n",
    "# Create the third bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_labels_100, binned_counts_100, width=0.6, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Unique Values in Columns (0-100)')\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.title('Columns Grouped by Number of Unique Values (20 Bins, 0-100)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given number of unique value mainly in 0-5 range, lets say it's categorical if in this range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_threshold = 5\n",
    "\n",
    "# Step 1: Compute unique value counts for each integer column\n",
    "unique_value_counts = np.array([len(np.unique(x_train_cleaned[:, col])) for col in integer_columns])\n",
    "\n",
    "# Step 2: Identify categorical and non-categorical features based on the threshold\n",
    "indexes_categorical_features = [integer_columns[i] for i, count in enumerate(unique_value_counts) if count <= categorical_threshold]\n",
    "indexes_non_categorical_features = [integer_columns[i] for i in range(len(unique_value_counts)) if integer_columns[i] not in indexes_categorical_features]\n",
    "\n",
    "assert len(indexes_categorical_features) + len(indexes_non_categorical_features) == len(unique_value_counts)\n",
    "assert unique_value_counts.size == len(integer_columns)\n",
    "\n",
    "indexes_non_categorical_features.extend(non_integer_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_standardized = standardize_columns(x_train_cleaned_without_nans, range(x_train_cleaned_without_nans.shape[1]))\n",
    "\n",
    "x_standardized = standardize_columns(x_train_cleaned_without_nans, indexes_non_categorical_features)\n",
    "\n",
    "x_test_standardized = standardize_columns(adapted_x_test_without_nans, indexes_non_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_x_train = binary_encode_columns(x_standardized, indexes_categorical_features)\n",
    "\n",
    "encoded_x_train, encoded_x_test = consistent_binary_encode(x_standardized, x_test_standardized, indexes_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
    "print(encoded_x_train.shape)\n",
    "print(encoded_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Feature Selection Based on Correlation:\n",
    "X_ici = encoded_x_train\n",
    "y_ici = y_balanced\n",
    "\n",
    "initial_w = np.zeros(X_ici.shape[1])\n",
    "max_iters = 30\n",
    "gamma = 0.01\n",
    "\n",
    "# Sample usage\n",
    "X_reduced, removed_features = remove_highly_correlated_features(X_ici, threshold=0.9) # 0.9=high, 0.8=moderate, 0.5-0.7=low\n",
    "print(X_ici.shape)\n",
    "print(\"Reduced feature matrix shape:\", X_reduced.shape)\n",
    "\n",
    "w,loss = least_squares(y_ici, X_reduced)\n",
    "print(loss)\n",
    "\n",
    "w,loss = ridge_regression(y_ici, X_reduced, 0.1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.671096204063807\n",
      "Current iteration=1, loss=0.6509922894346324\n",
      "Current iteration=2, loss=0.6325905761376068\n",
      "Current iteration=3, loss=0.6156802997874721\n",
      "Current iteration=4, loss=0.6000811839925937\n",
      "Current iteration=5, loss=0.5856395148616139\n",
      "Current iteration=6, loss=0.5722243007829384\n",
      "Current iteration=7, loss=0.5597237576623368\n",
      "Current iteration=8, loss=0.5480422226743685\n",
      "Current iteration=9, loss=0.5370975138107107\n",
      "Current iteration=10, loss=0.5268187080143704\n",
      "Current iteration=11, loss=0.5171442914201612\n",
      "Current iteration=12, loss=0.5080206297099676\n",
      "Current iteration=13, loss=0.4994007080943082\n",
      "Current iteration=14, loss=0.4912430951985806\n",
      "Current iteration=15, loss=0.483511091111329\n",
      "Current iteration=16, loss=0.47617202594861285\n",
      "Current iteration=17, loss=0.4691966809548033\n",
      "Current iteration=18, loss=0.4625588091588316\n",
      "Current iteration=19, loss=0.4562347368723188\n",
      "Current iteration=20, loss=0.45020303087978675\n",
      "Current iteration=21, loss=0.4444442191021295\n",
      "Current iteration=22, loss=0.43894055489924044\n",
      "Current iteration=23, loss=0.4336758171032472\n",
      "Current iteration=24, loss=0.4286351394204999\n",
      "Current iteration=25, loss=0.42380486407854223\n",
      "Current iteration=26, loss=0.41917241558333096\n",
      "Current iteration=27, loss=0.414726191241336\n",
      "Current iteration=28, loss=0.41045546573118286\n",
      "Current iteration=29, loss=0.4063503075127678\n",
      "Current iteration=30, loss=0.40240150526438395\n",
      "Current iteration=31, loss=0.3986005028611526\n",
      "Current iteration=32, loss=0.3949393416674668\n",
      "Current iteration=33, loss=0.39141060912528414\n",
      "Current iteration=34, loss=0.38800739278930174\n",
      "Current iteration=35, loss=0.3847232390974258\n",
      "Current iteration=36, loss=0.38155211627698055\n",
      "Current iteration=37, loss=0.3784883808788519\n",
      "Current iteration=38, loss=0.37552674750723924\n",
      "Current iteration=39, loss=0.3726622613750953\n",
      "Current iteration=40, loss=0.3698902733671665\n",
      "Current iteration=41, loss=0.36720641733582426\n",
      "Current iteration=42, loss=0.3646065893911817\n",
      "Current iteration=43, loss=0.362086928977613\n",
      "Current iteration=44, loss=0.35964380155472864\n",
      "Current iteration=45, loss=0.3572737827229558\n",
      "Current iteration=46, loss=0.3549736436527716\n",
      "Current iteration=47, loss=0.35274033769287505\n",
      "Current iteration=48, loss=0.35057098804661957\n",
      "Current iteration=49, loss=0.3484628764181664\n",
      "Current iteration=50, loss=0.3464134325404156\n",
      "Current iteration=51, loss=0.34442022450600585\n",
      "Current iteration=52, loss=0.3424809498307905\n",
      "Current iteration=53, loss=0.3405934271863197\n",
      "Current iteration=54, loss=0.3387555887441634\n",
      "Current iteration=55, loss=0.33696547308047353\n",
      "Current iteration=56, loss=0.3352212185941523\n",
      "Current iteration=57, loss=0.33352105739635723\n",
      "Current iteration=58, loss=0.3318633096330557\n",
      "Current iteration=59, loss=0.3302463782058055\n",
      "Current iteration=60, loss=0.3286687438591223\n",
      "Current iteration=61, loss=0.3271289606055915\n",
      "Current iteration=62, loss=0.3256256514624565\n",
      "Current iteration=63, loss=0.32415750447567787\n",
      "Current iteration=64, loss=0.32272326900954074\n",
      "Current iteration=65, loss=0.32132175228173665\n",
      "Current iteration=66, loss=0.31995181612559737\n",
      "Current iteration=67, loss=0.31861237396257097\n",
      "Current iteration=68, loss=0.3173023879695734\n",
      "Current iteration=69, loss=0.31602086642697924\n",
      "Current iteration=70, loss=0.3147668612342419\n",
      "Current iteration=71, loss=0.31353946558116014\n",
      "Current iteration=72, loss=0.3123378117637299\n",
      "Current iteration=73, loss=0.3111610691344809\n",
      "Current iteration=74, loss=0.31000844217783896\n",
      "Current iteration=75, loss=0.30887916870195153\n",
      "Current iteration=76, loss=0.30777251813896317\n",
      "Current iteration=77, loss=0.30668778994636164\n",
      "Current iteration=78, loss=0.30562431210259666\n",
      "Current iteration=79, loss=0.3045814396906833\n",
      "Current iteration=80, loss=0.30355855356389677\n",
      "Current iteration=81, loss=0.302555059088237\n",
      "Current iteration=82, loss=0.3015703849565894\n",
      "Current iteration=83, loss=0.30060398206995065\n",
      "Current iteration=84, loss=0.2996553224814611\n",
      "Current iteration=85, loss=0.29872389839914987\n",
      "Current iteration=86, loss=0.2978092212437486\n",
      "Current iteration=87, loss=0.29691082075813624\n",
      "Current iteration=88, loss=0.2960282441650682\n",
      "Current iteration=89, loss=0.29516105537038645\n",
      "Current iteration=90, loss=0.29430883420877535\n",
      "Current iteration=91, loss=0.29347117572955506\n",
      "Current iteration=92, loss=0.2926476895200463\n",
      "Current iteration=93, loss=0.2918379990642672\n",
      "Current iteration=94, loss=0.2910417411348986\n",
      "Current iteration=95, loss=0.2902585652164773\n",
      "Current iteration=96, loss=0.2894881329580362\n",
      "Current iteration=97, loss=0.2887301176534608\n",
      "Current iteration=98, loss=0.28798420374797457\n",
      "Current iteration=99, loss=0.2872500863692442\n",
      "Current iteration=100, loss=0.28652747088167335\n",
      "Current iteration=101, loss=0.2858160724626104\n",
      "Current iteration=102, loss=0.28511561569924343\n",
      "Current iteration=103, loss=0.2844258342050253\n",
      "Current iteration=104, loss=0.28374647025444677\n",
      "Current iteration=105, loss=0.28307727443536335\n",
      "Current iteration=106, loss=0.28241800531769434\n",
      "Current iteration=107, loss=0.28176842913776107\n",
      "Current iteration=108, loss=0.2811283194972927\n",
      "Current iteration=109, loss=0.2804974570764903\n",
      "Current iteration=110, loss=0.27987562936020094\n",
      "Current iteration=111, loss=0.27926263037664595\n",
      "Current iteration=112, loss=0.2786582604480281\n",
      "Current iteration=113, loss=0.27806232595232144\n",
      "Current iteration=114, loss=0.27747463909580083\n",
      "Current iteration=115, loss=0.27689501769561214\n",
      "Current iteration=116, loss=0.2763232849719455\n",
      "Current iteration=117, loss=0.2757592693493861\n",
      "Current iteration=118, loss=0.275202804266819\n",
      "Current iteration=119, loss=0.2746537279956423\n",
      "Current iteration=120, loss=0.2741118834657147\n",
      "Current iteration=121, loss=0.2735771180988256\n",
      "Current iteration=122, loss=0.2730492836490571\n",
      "Current iteration=123, loss=0.27252823605008886\n",
      "Current iteration=124, loss=0.27201383526869\n",
      "Current iteration=125, loss=0.2715059451644302\n",
      "Current iteration=126, loss=0.2710044333551012\n",
      "Current iteration=127, loss=0.2705091710877247\n",
      "Current iteration=128, loss=0.2700200331147245\n",
      "Current iteration=129, loss=0.2695368975752692\n",
      "Current iteration=130, loss=0.2690596458811686\n",
      "Current iteration=131, loss=0.2685881626075582\n",
      "Current iteration=132, loss=0.26812233538774555\n",
      "Current iteration=133, loss=0.2676620548123358\n",
      "Current iteration=134, loss=0.2672072143322059\n",
      "Current iteration=135, loss=0.26675771016537414\n",
      "Current iteration=136, loss=0.2663134412073535\n",
      "Current iteration=137, loss=0.2658743089450667\n",
      "Current iteration=138, loss=0.265440217374004\n",
      "Current iteration=139, loss=0.2650110729185289\n",
      "Current iteration=140, loss=0.26458678435517374\n",
      "Current iteration=141, loss=0.26416726273891167\n",
      "Current iteration=142, loss=0.2637524213320697\n",
      "Current iteration=143, loss=0.26334217553595335\n",
      "Current iteration=144, loss=0.2629364428249367\n",
      "Current iteration=145, loss=0.2625351426830445\n",
      "Current iteration=146, loss=0.2621381965427783\n",
      "Current iteration=147, loss=0.2617455277262222\n",
      "Current iteration=148, loss=0.2613570613881876\n",
      "Current iteration=149, loss=0.2609727244614715\n",
      "Current iteration=150, loss=0.2605924456040514\n",
      "Current iteration=151, loss=0.260216155148128\n",
      "Current iteration=152, loss=0.2598437850509508\n",
      "Current iteration=153, loss=0.25947526884741123\n",
      "Current iteration=154, loss=0.259110541604267\n",
      "Current iteration=155, loss=0.25874953987593635\n",
      "Current iteration=156, loss=0.25839220166180765\n",
      "Current iteration=157, loss=0.258038466365064\n",
      "Current iteration=158, loss=0.25768827475284517\n",
      "Current iteration=159, loss=0.2573415689178031\n",
      "Current iteration=160, loss=0.2569982922409201\n",
      "Current iteration=161, loss=0.2566583893556123\n",
      "Current iteration=162, loss=0.25632180611298294\n",
      "Current iteration=163, loss=0.25598848954823267\n",
      "Current iteration=164, loss=0.2556583878482508\n",
      "Current iteration=165, loss=0.25533145032014637\n",
      "Current iteration=166, loss=0.25500762736089044\n",
      "Current iteration=167, loss=0.25468687042794164\n",
      "Current iteration=168, loss=0.25436913201077155\n",
      "Current iteration=169, loss=0.2540543656033223\n",
      "Current iteration=170, loss=0.25374252567737793\n",
      "Current iteration=171, loss=0.2534335676567366\n",
      "Current iteration=172, loss=0.2531274478922211\n",
      "Current iteration=173, loss=0.2528241236374572\n",
      "Current iteration=174, loss=0.25252355302542784\n",
      "Current iteration=175, loss=0.2522256950457453\n",
      "Current iteration=176, loss=0.2519305095226527\n",
      "Current iteration=177, loss=0.25163795709361875\n",
      "Current iteration=178, loss=0.2513479991887287\n",
      "Current iteration=179, loss=0.25106059801055675\n",
      "Current iteration=180, loss=0.2507757165147469\n",
      "Current iteration=181, loss=0.25049331839113503\n",
      "Current iteration=182, loss=0.2502133680454529\n",
      "Current iteration=183, loss=0.24993583058159724\n",
      "Current iteration=184, loss=0.24966067178434764\n",
      "Current iteration=185, loss=0.24938785810270334\n",
      "Current iteration=186, loss=0.24911735663365575\n",
      "Current iteration=187, loss=0.248849135106389\n",
      "Current iteration=188, loss=0.24858316186705934\n",
      "Current iteration=189, loss=0.2483194058638837\n",
      "Current iteration=190, loss=0.2480578366327632\n",
      "Current iteration=191, loss=0.24779842428327434\n",
      "Current iteration=192, loss=0.24754113948503143\n",
      "Current iteration=193, loss=0.2472859534545118\n",
      "Current iteration=194, loss=0.2470328379421871\n",
      "Current iteration=195, loss=0.2467817652200394\n",
      "Current iteration=196, loss=0.24653270806945213\n",
      "Current iteration=197, loss=0.246285639769395\n",
      "Current iteration=198, loss=0.24604053408497012\n",
      "Current iteration=199, loss=0.2457973652562527\n",
      "Current iteration=200, loss=0.24555610798747354\n",
      "Current iteration=201, loss=0.24531673743644453\n",
      "Current iteration=202, loss=0.24507922920432415\n",
      "Current iteration=203, loss=0.24484355932563023\n",
      "Current iteration=204, loss=0.2446097042585293\n",
      "Current iteration=205, loss=0.24437764087539265\n",
      "Current iteration=206, loss=0.24414734645361885\n",
      "Current iteration=207, loss=0.2439187986666393\n",
      "Current iteration=208, loss=0.24369197557526562\n",
      "Current iteration=209, loss=0.24346685561915696\n",
      "Current iteration=210, loss=0.24324341760859267\n",
      "Current iteration=211, loss=0.24302164071642318\n",
      "Current iteration=212, loss=0.24280150447022916\n",
      "Current iteration=213, loss=0.24258298874470882\n",
      "Current iteration=214, loss=0.24236607375421912\n",
      "Current iteration=215, loss=0.24215074004557813\n",
      "Current iteration=216, loss=0.24193696849095817\n",
      "Current iteration=217, loss=0.24172474028106525\n",
      "Current iteration=218, loss=0.24151403691839332\n",
      "Current iteration=219, loss=0.24130484021072438\n",
      "Current iteration=220, loss=0.2410971322647303\n",
      "Current iteration=221, loss=0.24089089547980783\n",
      "Current iteration=222, loss=0.24068611254200215\n",
      "Current iteration=223, loss=0.24048276641809588\n",
      "Current iteration=224, loss=0.24028084034988617\n",
      "Current iteration=225, loss=0.24008031784854098\n",
      "Current iteration=226, loss=0.23988118268914355\n",
      "Current iteration=227, loss=0.23968341890533812\n",
      "Current iteration=228, loss=0.23948701078413243\n",
      "Current iteration=229, loss=0.23929194286080066\n",
      "Current iteration=230, loss=0.23909819991393683\n",
      "Current iteration=231, loss=0.2389057669605863\n",
      "Current iteration=232, loss=0.2387146292515549\n",
      "Current iteration=233, loss=0.23852477226676286\n",
      "Current iteration=234, loss=0.2383361817107616\n",
      "Current iteration=235, loss=0.23814884350832133\n",
      "Current iteration=236, loss=0.23796274380015456\n",
      "Current iteration=237, loss=0.2377778689387105\n",
      "Current iteration=238, loss=0.23759420548408683\n",
      "Current iteration=239, loss=0.23741174020001743\n",
      "Current iteration=240, loss=0.23723046004998902\n",
      "Current iteration=241, loss=0.23705035219340395\n",
      "Current iteration=242, loss=0.2368714039818654\n",
      "Current iteration=243, loss=0.2366936029555164\n",
      "Current iteration=244, loss=0.2365169368395107\n",
      "Current iteration=245, loss=0.23634139354049508\n",
      "Current iteration=246, loss=0.2361669611432474\n",
      "Current iteration=247, loss=0.23599362790732792\n",
      "Current iteration=248, loss=0.23582138226383717\n",
      "Current iteration=249, loss=0.23565021281224924\n",
      "Current iteration=250, loss=0.2354801083172919\n",
      "Current iteration=251, loss=0.23531105770592314\n",
      "Current iteration=252, loss=0.235143050064359\n",
      "Current iteration=253, loss=0.23497607463516526\n",
      "Current iteration=254, loss=0.2348101208144335\n",
      "Current iteration=255, loss=0.23464517814897856\n",
      "Current iteration=256, loss=0.23448123633364912\n",
      "Current iteration=257, loss=0.23431828520865147\n",
      "Current iteration=258, loss=0.23415631475695697\n",
      "Current iteration=259, loss=0.23399531510176566\n",
      "Current iteration=260, loss=0.23383527650399402\n",
      "Current iteration=261, loss=0.23367618935986698\n",
      "Current iteration=262, loss=0.23351804419851901\n",
      "Current iteration=263, loss=0.23336083167965918\n",
      "Current iteration=264, loss=0.23320454259129136\n",
      "Current iteration=265, loss=0.23304916784748453\n",
      "Current iteration=266, loss=0.23289469848617567\n",
      "Current iteration=267, loss=0.23274112566702584\n",
      "Current iteration=268, loss=0.23258844066933612\n",
      "Current iteration=269, loss=0.23243663488997263\n",
      "Current iteration=270, loss=0.232285699841374\n",
      "Current iteration=271, loss=0.23213562714957003\n",
      "Current iteration=272, loss=0.23198640855225408\n",
      "Current iteration=273, loss=0.23183803589689397\n",
      "Current iteration=274, loss=0.23169050113888093\n",
      "Current iteration=275, loss=0.2315437963397183\n",
      "Current iteration=276, loss=0.23139791366523538\n",
      "Current iteration=277, loss=0.23125284538386098\n",
      "Current iteration=278, loss=0.2311085838649024\n",
      "Current iteration=279, loss=0.23096512157688293\n",
      "Current iteration=280, loss=0.23082245108590405\n",
      "Current iteration=281, loss=0.23068056505403162\n",
      "Current iteration=282, loss=0.23053945623773106\n",
      "Current iteration=283, loss=0.23039911748632427\n",
      "Current iteration=284, loss=0.23025954174047114\n",
      "Current iteration=285, loss=0.23012072203069023\n",
      "Current iteration=286, loss=0.22998265147591188\n",
      "Current iteration=287, loss=0.22984532328204282\n",
      "Current iteration=288, loss=0.2297087307405752\n",
      "Current iteration=289, loss=0.2295728672272194\n",
      "Current iteration=290, loss=0.22943772620055003\n",
      "Current iteration=291, loss=0.22930330120069967\n",
      "Current iteration=292, loss=0.2291695858480645\n",
      "Current iteration=293, loss=0.2290365738420264\n",
      "Current iteration=294, loss=0.22890425895972552\n",
      "Current iteration=295, loss=0.22877263505482873\n",
      "Current iteration=296, loss=0.22864169605633514\n",
      "Current iteration=297, loss=0.2285114359674094\n",
      "Current iteration=298, loss=0.22838184886421967\n",
      "Current iteration=299, loss=0.228252928894814\n"
     ]
    }
   ],
   "source": [
    "# X_ici = x_train_cleaned_without_nans\n",
    "# X_ici = standardize_columns(X_ici, indexes_non_categorical_features)\n",
    "\n",
    "X_ici = encoded_x_train\n",
    "\n",
    "# # X_ici = x_train_cleaned\n",
    "\n",
    "y_ici = y_balanced\n",
    "\n",
    "# linear regression\n",
    "initial_w = np.zeros(X_ici.shape[1])\n",
    "max_iters = 300\n",
    "gamma = 0.01\n",
    "# print (least_squares(y_ici, X_ici))\n",
    "# w, loss = mean_squared_error_gd(y_ici, X_ici, initial_w, max_iters, gamma)\n",
    "\n",
    "\n",
    "w, loss = logistic_regression(y_ici, X_ici, initial_w, max_iters, gamma)\n",
    "\n",
    "# percentages_to_drop = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# nan_values_for_integer_columns = ['mode', 'upper', 'zero']\n",
    "# nan_values_for_continuous_columns = ['mean', 'mode', 'zero']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.09078236 -1.89824958 -2.11024134 ...  0.49096618  0.49433772\n",
      "  1.00909852]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.dot(encoded_x_train, w))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.dot(encoded_x_train, w), bins=50, edgecolor='black')\n",
    "plt.title('Reprojection of x_train before label prediction, balancing ratio 1.2')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9190524747038984 1.0\n"
     ]
    }
   ],
   "source": [
    "from prediction_score import *\n",
    "accuracy, f1_score = compute_scores(y_ici, X_ici, w)\n",
    "print(accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percentage of Nan to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_percentage_to_drop = None\n",
    "best_nan_value_for_integer_columns = None\n",
    "best_nan_value_for_continuous_columns = None\n",
    "\n",
    "for percentage_to_drop in percentages_to_drop:\n",
    "    for nan_value_for_continuous_columns in nan_values_for_continuous_columns:\n",
    "        for nan_value_for_integer_columns in nan_values_for_integer_columns:\n",
    "    \n",
    "            x_train_cleaned = remove_nan_features(x_train, percentage_to_drop)\n",
    "\n",
    "            x_train_cleaned_without_nans = encode_nan_integer_columns(x_train_cleaned, replacement_value=nan_value_for_integer_columns)\n",
    "            x_train_cleaned_without_nans = encode_nan_continuous_columns(x_train_cleaned_without_nans, replacement_value=nan_value_for_continuous_columns)\n",
    "\n",
    "            assert np.isnan(x_train_cleaned_without_nans).sum() == 0\n",
    "            assert x_train_cleaned.shape == x_train_cleaned_without_nans.shape\n",
    "\n",
    "            x_standardized = standardize_columns(x_train_cleaned_without_nans, range(x_train_cleaned_without_nans.shape[1]))\n",
    "\n",
    "            initial_w = np.zeros(X_ici.shape[1])\n",
    "            max_iters = 5000\n",
    "            gamma = 0.1\n",
    "\n",
    "            w, loss = mean_squared_error_gd(y_ici, X_ici, initial_w, max_iters, gamma)\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_percentage_to_drop = percentage_to_drop\n",
    "                best_nan_value_for_integer_columns = nan_value_for_integer_columns\n",
    "                best_nan_value_for_continuous_columns = nan_value_for_continuous_columns\n",
    "\n",
    "print(f\"Best loss: {best_loss}\")    \n",
    "print(f\"Best percentage to drop: {best_percentage_to_drop}\")\n",
    "print(f\"Best nan value for integer columns: {best_nan_value_for_integer_columns}\")\n",
    "print(f\"Best nan value for continuous columns: {best_nan_value_for_continuous_columns}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
