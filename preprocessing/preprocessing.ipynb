{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from helpers import *\n",
    "from helpers_perso import *\n",
    "from nan_imputation import *\n",
    "from one_hot_encoding import *\n",
    "from implementations import *\n",
    "from standardization import *\n",
    "from class_balancing import *\n",
    "from remove_highly_correlated_features import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\", \"dataset\")\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "print(\"Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_variances = np.nanvar(x_train, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# First subplot: Full range of variances with more bins\n",
    "axes[0].hist(column_variances, bins=100, edgecolor='black')\n",
    "axes[0].set_title(\"Distribution of Variance Across Columns (Full Range)\")\n",
    "axes[0].set_xlabel(\"Variance\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Second subplot: Focus on variance between 0 and 0.5\n",
    "axes[1].hist(column_variances, bins=20, range=(0, 1000), edgecolor='black')\n",
    "axes[1].set_title(\"Distribution of Variance Across Columns (0 to 1000)\")\n",
    "axes[1].set_xlabel(\"Variance\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Third subplot: Focus on variance between 1 and 200\n",
    "axes[2].hist(column_variances, bins=100, range=(1, 200), edgecolor='black')\n",
    "axes[2].set_title(\"Distribution of Variance Across Columns (1 to 200)\")\n",
    "axes[2].set_xlabel(\"Variance\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_ids = np.where(y_train == -1)[0]\n",
    "\n",
    "x_train_majority_class = x_train[class1_ids]\n",
    "\n",
    "# Calculate the proportion of NaN values in each column\n",
    "nan_proportions = np.isnan(x_train_majority_class).mean(axis=1)\n",
    "\n",
    "# Print the total number of columns plotted\n",
    "total_columns = nan_proportions.size\n",
    "print(f\"Total number of columns plotted: {total_columns}\")\n",
    "\n",
    "# Print the number of columns containing NaN values\n",
    "num_columns_with_nans = np.sum(nan_proportions > 0)\n",
    "print(f\"Number of columns containing NaN values: {num_columns_with_nans}\")\n",
    "\n",
    "# Define the bins for the histogram\n",
    "bins = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "\n",
    "# Calculate the histogram\n",
    "hist, bin_edges = np.histogram(nan_proportions, bins=bins)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(range(len(hist)), hist, tick_label=[f'{int(b*100)}-{int(bins[i+1]*100)}%' for i, b in enumerate(bins[:-1])])\n",
    "plt.xlabel('Proportion of NaN values')\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Proportion of Nan in participants of majority class (-1)')\n",
    "\n",
    "# Rotate the x-axis tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "balancing_ratio = 1.25\n",
    "x_balanced, y_balanced, deleted_ids = balance_classes(x_train, y_train, balancing_ratio)\n",
    "print(x_balanced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling columns containing Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of Nan values in Nan-containing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of NaN values in each column\n",
    "nan_proportions = np.isnan(x_train).mean(axis=0)\n",
    "\n",
    "# Print the total number of columns plotted\n",
    "total_columns = nan_proportions.size\n",
    "print(f\"Total number of columns plotted: {total_columns}\")\n",
    "\n",
    "# Print the number of columns containing NaN values\n",
    "num_columns_with_nans = np.sum(nan_proportions > 0)\n",
    "print(f\"Number of columns containing NaN values: {num_columns_with_nans}\")\n",
    "\n",
    "# Define the bins for the histogram\n",
    "bins = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "\n",
    "# Calculate the histogram\n",
    "hist, bin_edges = np.histogram(nan_proportions, bins=bins)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(range(len(hist)), hist, tick_label=[f'{int(b*100)}-{int(bins[i+1]*100)}%' for i, b in enumerate(bins[:-1])])\n",
    "plt.xlabel('Proportion of NaN values')\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Number of columns containing a proportion of NaN values')\n",
    "\n",
    "# Rotate the x-axis tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus not reasonnable to exclude columns containing Nan values\n",
    "Choice : remove columns with Nan proportion superior to 80 % ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all arrays by removing columns containing NaN values\n",
    "x_train_cleaned, deleted_indices = remove_nan_features(x_balanced, 0.8)\n",
    "\n",
    "adapted_x_test = np.delete(x_train, deleted_indices, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns, non_integer_columns = identify_integer_columns(x_train_cleaned)\n",
    "\n",
    "assert len(integer_columns) + len(non_integer_columns) == x_train_cleaned.shape[1]\n",
    "\n",
    "\n",
    "# Print the integer columns\n",
    "print(f\"Number of columns containing only integer values: {len(integer_columns)}\")\n",
    "\n",
    "# Count the number of columns in integer_columns that contain at least one zero\n",
    "num_columns_with_zero = sum(np.any(x_train_cleaned[:, col] == 0) for col in integer_columns)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of integer columns that contain at least one zero: {num_columns_with_zero/len(integer_columns)*100:.2f}%\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If only contains integers and no zeroes (=encoded), encode Nan as 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cleaned_without_nans = encode_nan_integer_columns(x_train_cleaned, replacement_value='mode')\n",
    "x_train_cleaned_without_nans = encode_nan_continuous_columns(x_train_cleaned_without_nans, replacement_value='mode')\n",
    "\n",
    "assert np.isnan(x_train_cleaned_without_nans).sum() == 0\n",
    "assert x_train_cleaned.shape == x_train_cleaned_without_nans.shape\n",
    "\n",
    "adapted_x_test_without_nans = encode_nan_integer_columns(adapted_x_test, replacement_value='mode')\n",
    "adapted_x_test_without_nans = encode_nan_continuous_columns(adapted_x_test_without_nans, replacement_value='mode')\n",
    "\n",
    "assert np.isnan(adapted_x_test_without_nans).sum() == 0\n",
    "assert adapted_x_test.shape == adapted_x_test_without_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of columns that do not contain only integer values\n",
    "num_non_integer_columns = len(non_integer_columns)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of columns that do not contain only integer values: {num_non_integer_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In columns containing only integer values, number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value_counts = np.array([len(np.unique(x_train_cleaned[:, col])) for col in integer_columns])\n",
    "\n",
    "# Create 20 bins based on the range of unique value counts\n",
    "max_unique = unique_value_counts.max() if unique_value_counts.size > 0 else 0\n",
    "bins = np.linspace(0, max_unique, 21)  # 21 edges for 20 bins\n",
    "bin_labels = [f'{int(b)}-{int(bins[i+1])}' for i, b in enumerate(bins[:-1])]\n",
    "\n",
    "# Count how many columns fall into each bin\n",
    "binned_counts = np.histogram(unique_value_counts, bins=bins)[0]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_labels, binned_counts, width=0.6, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Unique Values in Columns')\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.title('Columns Grouped by Number of Unique Values (20 Bins)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "# Create the second set of bins for values between 2000 and 0\n",
    "bins_2000 = np.linspace(0, 2000, 21)  # 21 edges for 20 bins\n",
    "bin_labels_2000 = [f'{int(b)}-{int(bins_2000[i+1])}' for i, b in enumerate(bins_2000[:-1])]\n",
    "\n",
    "# Count how many columns fall into each bin for the second set of bins\n",
    "binned_counts_2000 = np.histogram(unique_value_counts, bins=bins_2000)[0]\n",
    "\n",
    "# Create the second bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_labels_2000, binned_counts_2000, width=0.6, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Unique Values in Columns (0-2000)')\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.title('Columns Grouped by Number of Unique Values (20 Bins, 0-2000)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create the third set of bins for values between 0 and 100\n",
    "bins_100 = np.linspace(0, 100, 21)  # 21 edges for 20 bins\n",
    "bin_labels_100 = [f'{int(b)}-{int(bins_100[i+1])}' for i, b in enumerate(bins_100[:-1])]\n",
    "\n",
    "# Count how many columns fall into each bin for the third set of bins\n",
    "binned_counts_100 = np.histogram(unique_value_counts, bins=bins_100)[0]\n",
    "\n",
    "# Create the third bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_labels_100, binned_counts_100, width=0.6, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Unique Values in Columns (0-100)')\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.title('Columns Grouped by Number of Unique Values (20 Bins, 0-100)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given number of unique value mainly in 0-5 range, lets say it's categorical if in this range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_threshold = 5\n",
    "\n",
    "# Step 1: Compute unique value counts for each integer column\n",
    "unique_value_counts = np.array([len(np.unique(x_train_cleaned[:, col])) for col in integer_columns])\n",
    "\n",
    "# Step 2: Identify categorical and non-categorical features based on the threshold\n",
    "indexes_categorical_features = [integer_columns[i] for i, count in enumerate(unique_value_counts) if count <= categorical_threshold]\n",
    "indexes_non_categorical_features = [integer_columns[i] for i in range(len(unique_value_counts)) if integer_columns[i] not in indexes_categorical_features]\n",
    "\n",
    "assert len(indexes_categorical_features) + len(indexes_non_categorical_features) == len(unique_value_counts)\n",
    "assert unique_value_counts.size == len(integer_columns)\n",
    "\n",
    "indexes_non_categorical_features.extend(non_integer_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_standardized = standardize_columns(x_train_cleaned_without_nans, range(x_train_cleaned_without_nans.shape[1]))\n",
    "\n",
    "x_standardized = standardize_columns(x_train_cleaned_without_nans, indexes_non_categorical_features)\n",
    "\n",
    "x_test_standardized = standardize_columns(adapted_x_test_without_nans, indexes_non_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_x_train = binary_encode_columns(x_standardized, indexes_categorical_features)\n",
    "\n",
    "encoded_x_train, encoded_x_test = consistent_binary_encode(x_standardized, x_test_standardized, indexes_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_x_train.shape)\n",
    "print(encoded_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Feature Selection Based on Correlation:\n",
    "X_ici = encoded_x_train\n",
    "y_ici = y_balanced\n",
    "\n",
    "initial_w = np.zeros(X_ici.shape[1])\n",
    "max_iters = 30\n",
    "gamma = 0.01\n",
    "\n",
    "# Sample usage\n",
    "X_reduced, removed_features = remove_highly_correlated_features(X_ici, threshold=0.9) # 0.9=high, 0.8=moderate, 0.5-0.7=low\n",
    "print(X_ici.shape)\n",
    "print(\"Reduced feature matrix shape:\", X_reduced.shape)\n",
    "\n",
    "w,loss = least_squares(y_ici, X_reduced)\n",
    "print(loss)\n",
    "\n",
    "w,loss = ridge_regression(y_ici, X_reduced, 0.1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ici = encoded_x_train\n",
    "\n",
    "y_ici = y_balanced\n",
    "\n",
    "# Split the data into training and testing sets manually\n",
    "# Shuffle the rows of X_ici and y_ici in the same way\n",
    "shuffled_indices = np.random.permutation(X_ici.shape[0])\n",
    "X_ici = X_ici[shuffled_indices]\n",
    "y_ici = y_ici[shuffled_indices]\n",
    "\n",
    "split_index = int(0.8 * X_ici.shape[0])\n",
    "X_train_estim, X_test_estim = X_ici[:split_index], X_ici[split_index:]\n",
    "y_train_estim, y_test_estim = y_ici[:split_index], y_ici[split_index:]\n",
    "\n",
    "# linear regression\n",
    "initial_w = np.zeros(X_train_estim.shape[1])\n",
    "max_iters = 150\n",
    "gamma = 0.03\n",
    "# print (least_squares(y_ici, X_ici))\n",
    "# w, loss = mean_squared_error_gd(y_ici, X_ici, initial_w, max_iters, gamma)\n",
    "\n",
    "\n",
    "w, loss = logistic_regression(y_train_estim, X_train_estim, initial_w, max_iters, gamma)\n",
    "\n",
    "# percentages_to_drop = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# nan_values_for_integer_columns = ['mode', 'upper', 'zero']\n",
    "# nan_values_for_continuous_columns = ['mean', 'mode', 'zero']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the reprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log reg\n",
    "y_pred = sigmoid(np.dot(X_train_estim, w))\n",
    "maj_class = np.sum((y_pred >= 0) & (y_pred <= 0.5))\n",
    "min_class = np.sum((y_pred > 0.5) & (y_pred <= 1))\n",
    "ratio = maj_class/min_class\n",
    "print(maj_class, min_class, ratio)\n",
    "\n",
    "plt.text(0.95, 0.95, f'Ratio: {ratio:.2f}, expected {balancing_ratio}', transform=plt.gca().transAxes, \n",
    "        fontsize=12, verticalalignment='top', horizontalalignment='right', \n",
    "        bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.hist(y_pred, bins=50, edgecolor='black')\n",
    "plt.title(f'Reprojection of x_train before label prediction, balancing ratio {balancing_ratio}')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear\n",
    "y_pred = np.dot(X_train_estim, w)\n",
    "neg_values = np.sum(y_pred >= 0)\n",
    "pos_values = np.sum(y_pred < 0.5)\n",
    "ratio = neg_values/pos_values\n",
    "print(neg_values, pos_values, ratio)\n",
    "\n",
    "plt.text(0.95, 0.95, f'Ratio: {ratio:.2f}, expected {balancing_ratio}', transform=plt.gca().transAxes, \n",
    "        fontsize=12, verticalalignment='top', horizontalalignment='right', \n",
    "        bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.hist(y_pred, bins=50, edgecolor='black')\n",
    "plt.title(f'Reprojection of x_train before label prediction, balancing ratio {balancing_ratio}')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_score import *\n",
    "accuracy, f1_score = compute_scores(y_test_estim, X_test_estim, w)\n",
    "print(accuracy, f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
