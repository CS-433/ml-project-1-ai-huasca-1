{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "import os\n",
    "from helpers import *\n",
    "\n",
    "from crossvalidation import cross_validate\n",
    "from crossvalidation import tune_hyperparameters\n",
    "#from crossvalidation import tune_hyperparameters1\n",
    "#from crossvalidation import tune_hyperparameters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\", \"dataset\")\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "print(\"Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\", \"dataset\")\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "print(\"Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "#data_path = os.path.join(os.getcwd(), \"dataset\")\n",
    "# or\n",
    "# data_path = data_path = \"/Users/louistschanz/Documents/EPFL/MA1/ML/Project 1/ml-project-1-ai-huasca-1/dataset\"\n",
    "# x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "# print(\"Data loaded successfully!\")\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), \"data\", \"dataset\")\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path)\n",
    "print(\"Data loaded successfully!\")\n",
    "\n",
    "\n",
    "# Basic settings for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CODE\n",
    "# Create test arrays\n",
    "x_train = np.random.rand(150, 200)  # 25 x 25 array for features\n",
    "y_train = np.random.rand(150)   # 25 x 1 array for target values\n",
    "initial_weights = np.zeros(x_train.shape[1])  # Define initial weights based on feature count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs in x_train:\", np.isnan(x_train).any())\n",
    "print(\"Infs in x_train:\", np.isinf(x_train).any())\n",
    "print(\"NaNs in y_train:\", np.isnan(y_train).any())\n",
    "print(\"Infs in y_train:\", np.isinf(y_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models and Hyperparameter Grids\n",
    "# Dictionary of model functions\n",
    "models = {\n",
    "    \"mean_squared_error_gd\": mean_squared_error_gd,\n",
    "    \"mean_squared_error_sgd\": mean_squared_error_sgd,\n",
    "    \"least_squares\": least_squares,\n",
    "    \"ridge_regression\": ridge_regression,\n",
    "    \"logistic_regression\": logistic_regression,\n",
    "    \"reg_logistic_regression\": reg_logistic_regression\n",
    "}\n",
    "\n",
    "# # Define hyperparameter grids for each model\n",
    "param_grid = {\n",
    "    \"mean_squared_error_gd\": {\"max_iters\": [10], \"gamma\": [0.01, 0.001,0.0001]},\n",
    "    \"mean_squared_error_sgd\": {\"max_iters\": [10], \"gamma\": [0.01, 0.001, 0.0001]},\n",
    "    \"least_squares\": {},  # No hyperparameters for least squares\n",
    "    \"ridge_regression\": {\"lambda_\": [0.2, 0.1, 0.01, 0.001, 0.0001]},\n",
    "    \"logistic_regression\": {\"max_iters\": [10], \"gamma\": [0.01, 0.001, 0.0001]},\n",
    "    \"reg_logistic_regression\": {\"max_iters\": [10], \"gamma\": [0.01, 0.001, 0.0001], \"lambda_\": [0.2, 0.1, 0.01, 0.001, 0.0001]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning\n",
    "# Define models and parameter grid (as done before)\n",
    "# Example usage of the tune_hyperparameters function\n",
    "\n",
    "tuning_results = tune_hyperparameters(models, param_grid, x_train, y_train, initial_weights, k=5)\n",
    "#tuning_results = tune_hyperparameters1(models, param_grid, x_train, y_train, initial_weights, k=5)\n",
    "#tuning_results = tune_hyperparameters2(models, param_grid, x_train, y_train, k=5)\n",
    "\n",
    "# Print best results for each model\n",
    "for model_name, result in tuning_results.items():\n",
    "    print(f\"{model_name}: Best params: {result['best_params']}, Best score: {result['best_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) define the parameters\n",
    "initial_w = np.zeros(x_train.shape[1])  # Initial weights\n",
    "max_iters = 100                       # Number of iterations\n",
    "gamma = 0.1                             # Learning rate\n",
    "lambda_ = 0.1                           # Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) mean_squared_error_gd\n",
    "cv_loss_gd = cross_validate(model_fn=mean_squared_error_gd, X=x_train, y=y_train, k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma)\n",
    "print(\"Cross-validated loss for mean_squared_error_gd:\", cv_loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code que j'avais fais avant qd c'était dans le run, \n",
    "# mtn méthode differente, car run de crossvalidation dans le notebook crossvalidation\n",
    "\n",
    "\n",
    "# ***************** Cross validation ***************** #\n",
    "\n",
    "# Flexible crossvalidation on which function from implementations you want to validate\n",
    "# there are dynamical arguments and you can change the function you want to validate by changing the function name\n",
    "\n",
    "# 0) define the parameters\n",
    "initial_w = np.zeros(x_train.shape[1])  # Initial weights\n",
    "max_iters = 100                       # Number of iterations\n",
    "gamma = 0.1                             # Learning rate\n",
    "lambda_ = 0.1                           # Regularization parameter\n",
    "\n",
    "# 1) mean_squared_error_gd\n",
    "cv_loss_gd = cross_validate(model_fn=mean_squared_error_gd, X=x_train, y=y_train, k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma)\n",
    "print(\"Cross-validated loss for mean_squared_error_gd:\", cv_loss_gd)\n",
    "\n",
    "# 2) mean_squared_error_sgd\n",
    "cv_loss_sgd = cross_validate(model_fn=mean_squared_error_sgd, X=x_train, y=y_train, k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma)\n",
    "print(\"Cross-validated loss for mean_squared_error_sgd:\", cv_loss_gd)\n",
    "\n",
    "# 3) least_squares\n",
    "cv_loss_ls = cross_validate(model_fn=least_squares, X=x_train, Y=y_train, k=5)\n",
    "print(\"Cross-validated loss for least_squares:\", cv_loss_ls)\n",
    "\n",
    "# 4) ridge_regression\n",
    "#lambda_ = 0.1\n",
    "#cv_loss_rr = cross_validate(ridge_regression, X=x_train, Y=y_train, k=5, lambda_=lambda_)\n",
    "#print(\"Cross-validated loss for ridge_regression:\", cv_loss_rr)\n",
    "\n",
    "# 5) logistic_regression\n",
    "cv_loss_lr = cross_validate(\n",
    "    model_fn=logistic_regression,  # Pass the logistic regression function\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    k=5,\n",
    "    initial_w=initial_w,\n",
    "    max_iters=max_iters,\n",
    "    gamma=gamma\n",
    "    )\n",
    "print(\"Cross-validated loss for logistic_regression:\", cv_loss_lr)\n",
    "\n",
    "# 6) reg_logistic_regression\n",
    "cv_loss_rlr = cross_validate(\n",
    "    model_fn=reg_logistic_regression,  # Pass the regularized logistic regression function\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    k=5, initial_w=initial_w, max_iters=max_iters, gamma=gamma,lambda_=lambda_\n",
    ")\n",
    "print(\"Cross-validated loss for reg_logistic_regression:\", cv_loss_rlr)\n",
    "\n",
    "# 4) Ridge Regression : cross validate + fine tuning of parameters\n",
    "# Define ranges for hyperparameters\n",
    "gamma_values = [0.01, 0.1, 0.5]\n",
    "lambda_values = [0.01, 0.1, 1, 10]\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Perform grid search over gamma and lambda_\n",
    "for gamma in gamma_values:\n",
    "    for lambda_ in lambda_values:\n",
    "        cv_loss_rr = cross_validate(model_fn=ridge_regression,X=x_train,y=y_train,k=5,gamma=gamma,lambda_=lambda_)\n",
    "        print(f\"Cross-validated loss with gamma={gamma}, lambda_={lambda_}: {cv_loss_rr}\")\n",
    "        \n",
    "        # Update best score and parameters\n",
    "        if cv_loss_rr < best_score:\n",
    "            best_score = cv_loss_rr\n",
    "            best_params = {'gamma': gamma, 'lambda_': lambda_}\n",
    "\n",
    "print(f\"The best parameters: {best_params} yield a cross-validated loss of: {best_score}\")\n",
    "\n",
    "\n",
    "# LOOCV: To perform Leave-One-Out Cross-Validation --> just call cross_validate with k=len(y)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
